<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="David A Roberts">
  <title>Stochaskell</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="tufte-pandoc-css/tufte-css/tufte.css">
  <link rel="stylesheet" href="tufte-pandoc-css/pandoc.css">
  <link rel="stylesheet" href="tufte-pandoc-css/pandoc-solarized.css">
  <link rel="stylesheet" href="tufte-pandoc-css/tufte-extra.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") { katex.render(texText.data, mathElements[i], { displayMode: mathElements[i].classList.contains("display"), throwOnError: false } );
    }}});</script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<h1 class="title">Stochaskell</h1>
<p class="byline">David A Roberts</p>
</header>
<section>
<p>Probabilistic programming systems automate the task of translating specifications of probabilistic models into executable inference procedures. Here we present <em>Stochaskell</em>, an embedded domain-specific language for specifying probabilistic programs that can be compiled to multiple existing probabilistic programming languages. Programs are written as Haskell code, which transparently constructs an intermediate representation that is then passed to specialised code generation modules. This allows tight integration with existing probabilistic programming systems, with each system providing inference for different parameters of a model. These model-specific inference strategies are also written as Haskell code, providing a powerful means of structuring and composing inference methods.</p>
</section>
<section id="motivation" class="level2">
<h2>Motivation</h2>
<p>At one end of the probabilistic programming spectrum, systems like <a href="https://mc-stan.org/">Stan</a> produce high-performance inference programs utilising sampling algorithms such as Hamiltonian Monte Carlo (HMC). However, a consequence of this is that Stan does not support sampling discrete model parameters, requiring users to manually marginalise them out of the model prior to implementing it in Stan.<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">See <a href="https://mc-stan.org/docs/2_18/stan-users-guide/latent-discrete-chapter.html">§7 of <em>Stan User’s Guide</em></a>.<br />
<br />
</span></span> In particular, this precludes the ability to perform inference on models in which the dimensionality of the parameter space is itself a random variable. At the other end of the spectrum, implementations of Church <span class="citation" data-cites="goodman08">(Goodman et al. <a href="#ref-goodman08">2008</a>)</span> emphasise the ability to perform inference with few limitations on the models permitted.</p>
<p>Unfortunately, the model specification language is often tightly coupled with the system for translating the specification into an executable inference program. This requires users to rewrite their models in a number of different languages in order to fully take advantage of the variety of available probabilistic programming systems. Not only is this a burden on the user, but it also limits the ability to integrate a number of different probabilistic programming systems in the implementation of a single model. For instance, it may be desirable to infer continuous model parameters with Stan, whilst delegating the discrete and trans-dimensional parameters of the same model to a more universally applicable method.</p>
<p>Here we present Stochaskell, a probabilistic programming language designed for portability, allowing models and inference strategies to be written in a single language, but inference to be performed by a variety of probabilistic programming systems. This is achieved through runtime code generation, whereby code is automatically produced to perform inference via an external probabilistic programming system on subsets of the model specified by the user. In this way, users can benefit from the diverse probabilistic programming ecosystem without the cost of manually rewriting models in multiple different languages.</p>
<p>Many probabilistic programming systems — particularly those implementing the technique described by <span class="citation" data-cites="wingate11">Wingate, Stuhlmueller, and Goodman (<a href="#ref-wingate11">2011</a>)</span> — rely on code generation by translating probabilistic programs to a <em>general-purpose</em> programming language. Probabilistic C <span class="citation" data-cites="paige14">(Paige and Wood <a href="#ref-paige14">2014</a>)</span> goes further by positioning itself as a code generation <em>target</em> language for other probabilistic programming systems. Hakaru <span class="citation" data-cites="narayanan16">(Narayanan et al. <a href="#ref-narayanan16">2016</a>)</span> performs inference through program transformation, but both the source and target are expressed in the same probabilistic programming language. The Fun language <span class="citation" data-cites="borgstrom11">(Borgström et al. <a href="#ref-borgstrom11">2011</a>)</span> is implemented through compilation to Infer.NET programs. To the best of our knowledge, Stochaskell is the first probabilistic programming language capable of generating code for multiple probabilistic programming systems, spanning diverse probabilistic programming paradigms.</p>
</section>
<section id="probabilistic-modelling" class="level2">
<h2>Probabilistic Modelling</h2>
<p>In this section, we will introduce Stochaskell through a number of example probabilistic programs.<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">Also see <a href="https://www.haskell.org/tutorial/">A Gentle Introduction to Haskell</a> and the <a href="./doc/">Stochaskell API documentation</a>.<br />
<br />
</span></span> We begin with a simple program to simulate a homogeneous Poisson process over a fixed interval <span class="math inline">[0,t]</span>, using the standard method <span class="citation" data-cites="kroese11">(Kroese, Taimre, and Botev <a href="#ref-kroese11">2011</a>, sec. 5.4)</span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb1-1" title="1"><span class="ot">poissonProcess ::</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">P</span> (<span class="dt">Z</span>,<span class="dt">RVec</span>)</a>
<a class="sourceLine" id="cb1-2" title="2">poissonProcess rate t <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb1-3" title="3">  n <span class="ot">&lt;-</span> poisson (rate <span class="fu">*</span> t)</a>
<a class="sourceLine" id="cb1-4" title="4">  s <span class="ot">&lt;-</span> orderedSample n (uniform <span class="dv">0</span> t)</a>
<a class="sourceLine" id="cb1-5" title="5">  <span class="fu">return</span> (n,s)</a></code></pre></div>
<p>The program takes two real numbers as input<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Each represented by the type <code>R</code> in the type signature. Likewise, <code>Z</code> represents integers, <code>RVec</code> a vector of reals, and <code>P (Z,RVec)</code> a probability distribution over pairs of these.<br />
<br />
</span></span> specifying the <span class="math inline">\text{rate}&gt;0</span> of the process, and the length <span class="math inline">t&gt;0</span> of the interval, respectively. The number of points <span class="math inline">n</span> is simulated according to a Poisson distribution with mean <span class="math inline">\text{rate}\times t</span>, then the point locations are sampled uniformly from the interval <span class="math inline">[0,t]</span> and stored in an ordered vector <span class="math inline">s</span>. The output of the program is the (random) pair of these two values. We can run the simulation within a Haskell interactive environment, for example:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" title="1"><span class="fu">&gt;</span> simulate (poissonProcess <span class="dv">1</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb2-2" title="2">(<span class="dv">2</span>,[<span class="fl">1.2151445207978782</span>,<span class="fl">2.7518508992238075</span>])</a></code></pre></div>
<p>Of course, the real benefit of probabilistic programming is not just the ability to simulate data, but to infer model parameters given some observed data. However, we defer further discussion of this to <a href="#probabilistic-inference">the following section</a>, focusing for now on just the programs themselves.</p>
<p>We now consider the following program, which implements a Gaussian process (GP) model <span class="citation" data-cites="rasmussen06">(Rasmussen and Williams <a href="#ref-rasmussen06">2006</a>, sec. 2.2)</span>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb3-1" title="1"><span class="ot">gp ::</span> (<span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">R</span>) <span class="ot">-&gt;</span> <span class="dt">Z</span> <span class="ot">-&gt;</span> <span class="dt">RVec</span> <span class="ot">-&gt;</span> <span class="dt">P</span> <span class="dt">RVec</span></a>
<a class="sourceLine" id="cb3-2" title="2">gp kernel n x <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb3-3" title="3">  <span class="kw">let</span> mu <span class="fu">=</span> vector [ <span class="dv">0</span> <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n ]</a>
<a class="sourceLine" id="cb3-4" title="4">      cov <span class="fu">=</span> matrix [ kernel (x<span class="fu">!</span>i) (x<span class="fu">!</span>j) <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n, j <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n ]</a>
<a class="sourceLine" id="cb3-5" title="5">  g <span class="ot">&lt;-</span> normal mu cov</a>
<a class="sourceLine" id="cb3-6" title="6">  <span class="fu">return</span> g</a></code></pre></div>
<p>The definitions of <code>mu</code> and <code>cov</code> illustrate the usage of Stochaskell’s abstract arrays.<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">Using the <em>monad comprehensions</em> language extension.<br />
<br />
</span></span> In contrast to concrete arrays,<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">Such as those provided by the standard <code>Data.Array</code> module.<br />
<br />
</span></span> the dimensions of abstract arrays can be specified in terms of random variables. Array elements can be accessed using the conventional subscript operator, as witnessed by the expression <code>(x!i)</code> providing the value of the <code>i</code>th location at which we are evaluating the GP.</p>
<p>It is also worth drawing attention to the fact that the first input to this program is a <em>function</em> which takes two real locations and returns the desired covariance of the GP at those locations.<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">The kernel function should be symmetric and positive semidefinite <span class="citation" data-cites="rasmussen06">(Rasmussen and Williams <a href="#ref-rasmussen06">2006</a>, sec. 4.1)</span>.<br />
<br />
</span></span> This highlights an advantage of the embedded approach, whereby existing Haskell functions can be freely used within Stochaskell programs. For instance, in the following code,<span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">Here we are utilising the <a href="https://hackage.haskell.org/package/Boolean"><code>Data.Boolean.Overload</code> module</a> to ensure the definition is sufficiently polymorphic.<br />
<br />
</span></span> <code>kernelSE1</code> specifies a squared exponential kernel function with unit <em>signal variance</em> and <em>length-scale</em> hyper-parameters <span class="citation" data-cites="rasmussen06">(Rasmussen and Williams <a href="#ref-rasmussen06">2006</a>, sec. 4.2)</span>, and a small Gaussian observation noise of variance <span class="math inline">10^{-6}</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" title="1">kernelSE1 <span class="fu">=</span> kernelSE (<span class="fu">log</span> <span class="dv">1</span>) (<span class="fu">log</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb4-2" title="2"></a>
<a class="sourceLine" id="cb4-3" title="3">kernelSE lsv lls2 a b <span class="fu">=</span></a>
<a class="sourceLine" id="cb4-4" title="4">  <span class="fu">exp</span> (lsv <span class="fu">-</span> (a <span class="fu">-</span> b)<span class="fu">^</span><span class="dv">2</span> <span class="fu">/</span> (<span class="dv">2</span> <span class="fu">*</span> <span class="fu">exp</span> lls2))</a>
<a class="sourceLine" id="cb4-5" title="5">  <span class="fu">+</span> <span class="kw">if</span> a <span class="fu">==</span> b <span class="kw">then</span> <span class="fl">1e-6</span> <span class="kw">else</span> <span class="dv">0</span></a></code></pre></div>
<p>Although it is possible to use the provided <code>normal</code> primitive to sample from a multivariate Gaussian distribution, it is also possible to do so more explicitly by transforming a vector of (univariate) standard normal samples, which can be useful for fine-tuning inference efficiency. The following program defines such a transformation with the aid of a Cholesky decomposition <span class="citation" data-cites="kroese11">(Kroese, Taimre, and Botev <a href="#ref-kroese11">2011</a>, sec. 4.3)</span>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" title="1"><span class="ot">normalChol ::</span> <span class="dt">Z</span> <span class="ot">-&gt;</span> <span class="dt">RVec</span> <span class="ot">-&gt;</span> <span class="dt">RMat</span> <span class="ot">-&gt;</span> <span class="dt">P</span> <span class="dt">RVec</span></a>
<a class="sourceLine" id="cb5-2" title="2">normalChol n mu cov <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb5-3" title="3">  w <span class="ot">&lt;-</span> joint vector [ normal <span class="dv">0</span> <span class="dv">1</span> <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n ]</a>
<a class="sourceLine" id="cb5-4" title="4">  <span class="fu">return</span> (mu <span class="fu">+</span> chol cov <span class="fu">#&gt;</span> w)</a></code></pre></div>
<p>The <code>joint</code> keyword indicates that we are sampling a random vector whose elements have the given marginal distributions. Note that, although the elements of this random vector are (conditionally) independent, correlations can be induced by applying a deterministic transformation, as with the matrix-vector multiplication (<code>#&gt;</code>) in the above program.</p>
<p>Building upon this, we can write a GP classification model <span class="citation" data-cites="rasmussen06">(Rasmussen and Williams <a href="#ref-rasmussen06">2006</a>, sec. 3.3)</span>, where <code>bernoulliLogit</code> represents the logit-parametrised <span class="math inline">\mathrm{Bernoulli}(\mathrm{logit}^{-1}(\cdot))</span> distribution:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" title="1"><span class="ot">gpClassifier ::</span> (<span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">R</span>) <span class="ot">-&gt;</span> <span class="dt">Z</span> <span class="ot">-&gt;</span> <span class="dt">RVec</span> <span class="ot">-&gt;</span> <span class="dt">P</span> (<span class="dt">RVec</span>,<span class="dt">BVec</span>)</a>
<a class="sourceLine" id="cb6-2" title="2">gpClassifier kernel n x <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb6-3" title="3">  g <span class="ot">&lt;-</span> gpChol kernel n x</a>
<a class="sourceLine" id="cb6-4" title="4">  phi <span class="ot">&lt;-</span> joint vector [ bernoulliLogit (g<span class="fu">!</span>i) <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n ]</a>
<a class="sourceLine" id="cb6-5" title="5">  <span class="fu">return</span> (g,phi)</a></code></pre></div>
<p>In this example, <code>gpChol</code> is itself a probabilistic program, defined much like the <code>gp</code> program earlier, but utilising the <code>normalChol</code> program instead of the <code>normal</code> primitive. This demonstrates how sub-programs can be easily composed in Stochaskell, another benefit we obtain for free from embedding.</p>
<p>Now for a somewhat different example, this program implements the stick-breaking process of <span class="citation" data-cites="sethuraman94">Sethuraman (<a href="#ref-sethuraman94">1994</a>)</span> with concentration parameter <span class="math inline">\alpha&gt;0</span>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" title="1"><span class="ot">stickBreak ::</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">P</span> <span class="dt">RVec</span></a>
<a class="sourceLine" id="cb7-2" title="2">stickBreak alpha <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb7-3" title="3">  sticks <span class="ot">&lt;-</span> joint vector [ beta <span class="dv">1</span> alpha <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>infinity ]</a>
<a class="sourceLine" id="cb7-4" title="4">  <span class="kw">let</span> sticks&#39; <span class="fu">=</span> vector [ <span class="dv">1</span> <span class="fu">-</span> (sticks<span class="fu">!</span>i) <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>infinity ] </a>
<a class="sourceLine" id="cb7-5" title="5">      rems <span class="fu">=</span> scan (<span class="fu">*</span>) <span class="dv">1</span> sticks&#39;</a>
<a class="sourceLine" id="cb7-6" title="6">      probs <span class="fu">=</span> vector [ (sticks<span class="fu">!</span>i) <span class="fu">*</span> (rems<span class="fu">!</span>i) <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>infinity ]</a>
<a class="sourceLine" id="cb7-7" title="7">  <span class="fu">return</span> probs</a></code></pre></div>
<p>Here we are utilising the higher-order function <em>scan</em>. Much like the closely-related <em>fold</em> operation (sometimes called <em>reduce</em> or <em>accumulate</em>), scan allows us to recursively combine elements of an array. In this example, <code>rems</code> is a vector containing the cumulative product of <code>sticks'</code>. Although fold and scan provide only a restricted form of recursion, they are powerful enough to be able to implement practically useful models such as Dirichlet processes (DP), as demonstrated in the following program. Stochaskell supports using higher-order functions like these, rather than allowing unlimited recursion, in order to provide a balance between expressiveness and portability, in consideration of the fact that some probabilistic programming systems may support only limited forms of recursion, if at all.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb8-1" title="1"><span class="ot">dirichletProcess ::</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">P</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">P</span> (<span class="dt">P</span> <span class="dt">R</span>)</a>
<a class="sourceLine" id="cb8-2" title="2">dirichletProcess alpha base <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb8-3" title="3">  probs <span class="ot">&lt;-</span> stickBreak alpha</a>
<a class="sourceLine" id="cb8-4" title="4">  atoms <span class="ot">&lt;-</span> joint vector [ base <span class="fu">|</span> i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>infinity ]</a>
<a class="sourceLine" id="cb8-5" title="5">  <span class="kw">let</span> randomDistribution <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb8-6" title="6">        stick <span class="ot">&lt;-</span> pmf probs</a>
<a class="sourceLine" id="cb8-7" title="7">        <span class="fu">return</span> (atoms<span class="fu">!</span>stick)</a>
<a class="sourceLine" id="cb8-8" title="8">  <span class="fu">return</span> randomDistribution</a></code></pre></div>
<p>Note that we can see from the type signature that this program encodes a <em>distribution over distributions</em>. These random distributions can be used like any other sub-program, as shown in this implementation of a DP mixture model <span class="citation" data-cites="neal00">(Neal <a href="#ref-neal00">2000</a>)</span>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb9-1" title="1"><span class="ot">dpmm ::</span> <span class="dt">Z</span> <span class="ot">-&gt;</span> <span class="dt">P</span> <span class="dt">RVec</span></a>
<a class="sourceLine" id="cb9-2" title="2">dpmm n <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb9-3" title="3">  <span class="kw">let</span> base <span class="fu">=</span> uniform <span class="dv">0</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb9-4" title="4">  paramDist <span class="ot">&lt;-</span> dirichletProcess <span class="dv">5</span> base</a>
<a class="sourceLine" id="cb9-5" title="5">  params <span class="ot">&lt;-</span> joint vector [ paramDist <span class="fu">|</span> j <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n ]</a>
<a class="sourceLine" id="cb9-6" title="6">  values <span class="ot">&lt;-</span> joint vector [ normal (params<span class="fu">!</span>j) <span class="dv">1</span> <span class="fu">|</span> j <span class="ot">&lt;-</span> <span class="dv">1</span><span class="fu">...</span>n ]</a>
<a class="sourceLine" id="cb9-7" title="7">  <span class="fu">return</span> values</a></code></pre></div>
<section id="intermediate-representation" class="level3">
<h3>Intermediate Representation</h3>
<p>See <span class="citation" data-cites="roberts19">Roberts, Gallagher, and Taimre (<a href="#ref-roberts19">2019</a>, sec. 3.1)</span>.</p>
</section>
</section>
<section id="probabilistic-inference" class="level2">
<h2>Probabilistic Inference</h2>
<p>Statistical inference, within a Bayesian paradigm, involves the consideration of the posterior distribution of model parameters conditioned on observed data. For clarity, we will focus on the common case of performing inference by drawing approximate samples from the posterior via Markov chain Monte Carlo (MCMC) methods. However, note that Stochaskell does not restrict itself to this case, and integration with a wide range of different inference methods is possible. Posterior distributions can be expressed in Stochaskell with a familiar notation,<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote">Thanks to monad comprehensions, and a generalised <code>guard</code> function.<br />
<br />
</span></span> for instance given a probabilistic program <code>prior</code> and some <code>observed</code> data:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb10-1" title="1">posterior <span class="fu">=</span> [ z <span class="fu">|</span> (z,y) <span class="ot">&lt;-</span> prior, y <span class="fu">==</span> observed ]</a></code></pre></div>
<p>Stochaskell supports programmable inference, as introduced by <span class="citation" data-cites="mansinghka14">Mansinghka, Selsam, and Perov (<a href="#ref-mansinghka14">2014</a>)</span>, with inference strategies expressed in the same language as the model, i.e. Haskell. This allows us to seamlessly combine different inference methods, be they provided by native Haskell code or external probabilistic programming systems. In the latter case, runtime code generation allows us to offload inference of different parts of a model without any manual code duplication.</p>
<section id="stan-integration" class="level3">
<h3>Stan Integration</h3>
<p>Stochaskell integrates with Stan, to provide high-performance inference for (conditionally) fixed-dimensional, continuous model parameters. This is achieved by automatically translating Stochaskell programs into Stan code, compiling the resulting code with <a href="https://mc-stan.org/users/interfaces/cmdstan">CmdStan</a>, then executing the compiled model and parsing the results so that they can be further processed within Haskell. This allows the user to offload inference to Stan with a single line of code, receiving a list of posterior samples in return:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb11-1" title="1">samples <span class="ot">&lt;-</span> hmcStan posterior</a></code></pre></div>
<p>The IR outlined in <a href="#intermediate-representation">the previous section</a> can be translated to Stan code in a fairly straightforward manner. Each Apply node is translated to variable definition with an automatically generated identifier. Array definitions correspond to a for-loop, or a set of nested for-loops, which iterate over each possible index value and store the appropriate element value into an array. A Fold node produces code that first assigns the default value to a variable, then iteratively updates it by looping over the elements of the vector.</p>
<p>Similar backends are also provided for integration with <a href="https://docs.pymc.io/">PyMC3</a> and <a href="http://edwardlib.org/">Edward</a>.</p>
</section>
<section id="church-integration" class="level3">
<h3>Church Integration</h3>
<p>To demonstrate the portability of Stochaskell across a variety of probabilistic programming paradigms, we also provide integration with Church. The implementation and usage of this is quite similar to the Stan backend, allowing inference to be easily offloaded:<span><label for="sn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="marginnote"> <img src="imm.svg" /> Histogram of <span class="math inline">n=10^4</span> samples from the Dirichlet process mixture model program from <a href="#probabilistic-modelling">the previous section</a>, via the <a href="https://github.com/probmods/webchurch">WebChurch</a> implementation of Church.<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb12-1" title="1">samples <span class="ot">&lt;-</span> mhChurch posterior</a></code></pre></div>
<p>The main conceptual difference from the Stan code generation is that for Church, arrays are represented by <em>memoised functions</em>. In particular, random arrays — constructed with the <code>joint</code> keyword in Stochaskell — utilise Church’s support for <em>stochastic memoisation</em> <span class="citation" data-cites="goodman08">(Goodman et al. <a href="#ref-goodman08">2008</a>, sec. 2.1)</span>.</p>
<p>The figure on the right illustrates a simulation of the DP mixture model program presented in <a href="#probabilistic-modelling">the previous section</a>, sampled via the Church backend.</p>
</section>
<section id="metropolishastings" class="level3">
<h3>Metropolis–Hastings</h3>
<p>Stochaskell provides a native Haskell library for both simulating probabilistic programs, as well as computing the probability density function of the distribution they represent where possible. A notable feature of the latter is that we provide limited support for computing the density of transformed random variables. Suppose we have <span class="math inline">Z=h(X)</span>, where <span class="math inline">X</span> and <span class="math inline">Z</span> are real random vectors of common dimension. In the case that the transformation <span class="math inline">h</span> is invertible, the density of <span class="math inline">Z</span> can be computed from that of <span class="math inline">X</span> by applying the formula</p>
<p><span class="math display">p_Z(z)=\frac{p_X(h^{-1}(z))}{|J_h(h^{-1}(z))|}</span></p>
<p>where the denominator is the absolute value of the determinant of the Jacobian matrix of the transformation <span class="math inline">h</span> <span class="citation" data-cites="kroese11">(Kroese, Taimre, and Botev <a href="#ref-kroese11">2011</a>, sec. A.6)</span>. The Jacobian matrix is computed using automatic differentiation, and the inverse transform <span class="math inline">h^{-1}(z)</span> is calculated by the following procedure. Given an expression graph representing <span class="math inline">h(x)</span>, we associate the root node with the value of <span class="math inline">z</span>, then walk the graph associating values with nodes whenever they can be fully determined by the values of neighbouring nodes. When this process succeeds, the nodes representing <span class="math inline">x</span> will be associated with the value of <span class="math inline">h^{-1}(z)</span>.</p>
<p>This density computation library allows us to provide a simple implementation of the Metropolis–Hastings (M–H) inference algorithm, with support for <em>user-defined</em> proposal distributions:<span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote">Here <code>density</code> includes the Jacobian adjustment, whereas <code>density'</code> does not. The latter allows us to use proposals which make few random choices relative to the size of the state, under the assumption that they do not involve non-trivial transformations.<br />
<br />
<em>Note: these functions have been deprecated in favour of the more efficient <code>lpdf</code> and <code>lpdfAux</code> functions.</em><br />
<br />
</span></span></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb13-1" title="1">mh target proposal x <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb13-2" title="2">  y <span class="ot">&lt;-</span> simulate (proposal x)</a>
<a class="sourceLine" id="cb13-3" title="3">  <span class="kw">let</span> f <span class="fu">=</span> density target</a>
<a class="sourceLine" id="cb13-4" title="4">      q z <span class="fu">=</span> density&#39; (proposal z)</a>
<a class="sourceLine" id="cb13-5" title="5">      a <span class="fu">=</span> fromLogFloat <span class="fu">$</span></a>
<a class="sourceLine" id="cb13-6" title="6">        (f y <span class="fu">*</span> q y x) <span class="fu">/</span> (f x <span class="fu">*</span> q x y)</a>
<a class="sourceLine" id="cb13-7" title="7">  accept <span class="ot">&lt;-</span> bernoulli (<span class="kw">if</span> a <span class="fu">&gt;</span> <span class="dv">1</span> <span class="kw">then</span> <span class="dv">1</span> <span class="kw">else</span> a)</a>
<a class="sourceLine" id="cb13-8" title="8">  <span class="fu">return</span> (<span class="kw">if</span> accept <span class="kw">then</span> y <span class="kw">else</span> x)</a></code></pre></div>
<p>Proposals are specified by probabilistic programs written in Stochaskell, alongside the model. For instance, given the following random walk proposal:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb14-1" title="1"><span class="ot">proposal ::</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">P</span> <span class="dt">R</span></a>
<a class="sourceLine" id="cb14-2" title="2">proposal x <span class="fu">=</span> <span class="kw">do</span></a>
<a class="sourceLine" id="cb14-3" title="3">  z <span class="ot">&lt;-</span> normal <span class="dv">0</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb14-4" title="4">  <span class="fu">return</span> (x <span class="fu">+</span> z)</a></code></pre></div>
<p>we can sample from the distribution represented by some <code>program</code> via M–H as follows:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode hs"><code class="sourceCode haskell"><a class="sourceLine" id="cb15-1" title="1">x <span class="ot">&lt;-</span> chain n (program <span class="ot">`mh`</span> proposal) x0</a></code></pre></div>
<p>where <code>n</code> is the number of iterations and <code>x0</code> is some initial state.</p>
</section>
<section id="reversible-jump-markov-chain-monte-carlo" class="level3">
<h3>Reversible Jump Markov Chain Monte Carlo</h3>
<p>See <span class="citation" data-cites="roberts19">Roberts, Gallagher, and Taimre (<a href="#ref-roberts19">2019</a>)</span>.</p>
<!--
## Usage

### Docker

```sh
docker build -t stochaskell:latest .
docker run -it -p8888:8888 stochaskell:latest
```
-->
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references fullwidth">
<div id="ref-borgstrom11">
<p>Borgström, Johannes, Andrew D. Gordon, Michael Greenberg, James Margetson, and Jurgen Van Gael. 2011. “Measure Transformer Semantics for Bayesian Machine Learning.” In <em>Programming Languages and Systems</em>, 77–96. <a href="https://doi.org/10.1007/978-3-642-19718-5_5">https://doi.org/10.1007/978-3-642-19718-5_5</a>.</p>
</div>
<div id="ref-goodman08">
<p>Goodman, Noah D, Vikash K Mansinghka, Daniel M Roy, Keith Bonawitz, and Joshua B Tenenbaum. 2008. “Church: A Language for Generative Models.” In <em>Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence</em>, 220–29. <a href="https://dblp.org/rec/html/conf/uai/GoodmanMRBT08">https://dblp.org/rec/html/conf/uai/GoodmanMRBT08</a>.</p>
</div>
<div id="ref-kroese11">
<p>Kroese, Dirk P., Thomas Taimre, and Zdravko I. Botev. 2011. <em>Handbook of Monte Carlo Methods</em>. New York, New York: John Wiley and Sons. <a href="http://montecarlohandbook.org">http://montecarlohandbook.org</a>.</p>
</div>
<div id="ref-mansinghka14">
<p>Mansinghka, Vikash, Daniel Selsam, and Yura Perov. 2014. “Venture: A Higher-Order Probabilistic Programming Platform with Programmable Inference.” <a href="http://arxiv.org/abs/1404.0099">http://arxiv.org/abs/1404.0099</a>.</p>
</div>
<div id="ref-narayanan16">
<p>Narayanan, Praveen, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. 2016. “Probabilistic Inference by Program Transformation in Hakaru (System Description).” In <em>Proceedings of the 13th International Symposium on Functional and Logic Programming</em>, 62–79. <a href="https://doi.org/10.1007/978-3-319-29604-3_5">https://doi.org/10.1007/978-3-319-29604-3_5</a>.</p>
</div>
<div id="ref-neal00">
<p>Neal, Radford M. 2000. “Markov Chain Sampling Methods for Dirichlet Process Mixture Models.” <em>Journal of Computational and Graphical Statistics</em> 9 (2): 249–65. <a href="https://doi.org/10.1080/10618600.2000.10474879">https://doi.org/10.1080/10618600.2000.10474879</a>.</p>
</div>
<div id="ref-paige14">
<p>Paige, Brooks, and Frank Wood. 2014. “A Compilation Target for Probabilistic Programming Languages.” In <em>Proceedings of the 31st International Conference on Machine Learning</em>, 1935–43. <a href="http://jmlr.org/proceedings/papers/v32/paige14.html">http://jmlr.org/proceedings/papers/v32/paige14.html</a>.</p>
</div>
<div id="ref-rasmussen06">
<p>Rasmussen, C E, and C K I Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. University Press Group Limited. <a href="http://gaussianprocess.org/gpml">http://gaussianprocess.org/gpml</a>.</p>
</div>
<div id="ref-roberts19">
<p>Roberts, David A, Marcus Gallagher, and Thomas Taimre. 2019. “Reversible Jump Probabilistic Programming.” In <em>Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics</em>, to appear.</p>
</div>
<div id="ref-sethuraman94">
<p>Sethuraman, Jayaram. 1994. “A Constructive Definition of Dirichlet Priors.” <em>Statistica Sinica</em> 4 (2): 639–50. <a href="http://www.jstor.org/stable/24305538">http://www.jstor.org/stable/24305538</a>.</p>
</div>
<div id="ref-wingate11">
<p>Wingate, David, Andreas Stuhlmueller, and Noah D. Goodman. 2011. “Lightweight Implementations of Probabilistic Programming Languages via Transformational Compilation.” In <em>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</em>, 770–78. <a href="http://jmlr.org/proceedings/papers/v15/wingate11a.html">http://jmlr.org/proceedings/papers/v15/wingate11a.html</a>.</p>
</div>
</div>
</section>
</article>
</body>
</html>
